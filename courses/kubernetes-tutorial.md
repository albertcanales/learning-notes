# Kubernetes Tutorial

## Intro to K8s

Kubernetes does Container orchestration.

### Main K8s components

About pods and nodes:
- Node: Server which containers pods
- Pod: Abstract concept in top of container running on an isolated environment
- Each pod has its own IP address, so pods can communicate to each other
- When a pod dies and is restarted, a new IP will be assigned to it

To solve the last point, we introduce the Service:
- Permanent IP address that is attached to a pod
- Load balancing between nodes

Services can be:
- External: Accessible out of the node
- Internal: Accessible only in the node

But connecting to external services through Node's IP and Service's port is not feasable, thus we introduce the Ingress:
- Recieves the requests from outside
- Forwards the requests to the internal services

To introduce environment values to the pods (for example URLs to other pods) we can use ConfigMap. To store confidential data, we instead use the Secret component (is base64 encoded).

To store data, we use Volumes, which keep the data even after the pod's lifecicle. It can be inside or outside of the node.

To replicate pods, we create a Deployment for the pod. Thus we work with deployments, declaring the number of replicas in its configuration.

To manage Deployments for Stateful apps (like DB), we use a Stateful set. This will ensure that there won't be data inconsistencies. It may be difficult, so sometimes DB's are directly out of the cluster.

### K8s Arquitecture

Every node (or worker node) has 3 processes running:
- Container runtime: For example Docker
- Kubelet: Process that manages the pods using the container runtime
- Kube proxy: Forwards the requests between pods

To model the arquitecture and make the behavioural decisions for the worker nodes, we use the Master Node.

Every Master Node has:
- API Server: The API that we interact with to configure the cluster
- Scheduler: The program that assigns a new pod to a node and tells the Kubelet to start it
- Controller Manager: Detect changes in clusters (like a pod dying) and tries to recover the previous state (by requesting the Schedule, for example)
- etcd: Key-value storage of the cluster's state. Gets requests from the Scheduler to make decisions, for example

Due to the importance of the master node, it is replicated itself for redundancy.

For example, a small cluster may contain:
- 2 master nodes (low on resources)
- 3 worker nodes (higher in resources)

### Minikube and Kubectl

To manage the cluster we use the API Server (master process). We can interact with it by:
- UI: For example one of the dashboards
- API
- CLI: For example kubectl (most features)

Minikube allows use (for testing purposes, for example) to have both master and worker processes on the same machine.

### Kubectl commands

Get status of any component (node, pod, service...)

	kubectl get COMPONENT

Create a certain component

	kubectl create COMPONENT

We don't Create pods, we create Deployments (which is an abstraction above), giving the image:

	kubectl create deployment NAME --image=image [--options]

More complicated deployments will be seen afterwards, as this one gives almost everything to default.

This default configuration also creates a ReplicaSet, leaving the following structure: Container in Pod in ReplicaSet in Deployment. This is visible in the IDs. We just configure the Deployment and K8s manages the layers below.

Edit deployment's configuration (autogenerated on create):

	kubectl edit deployment NAME

After changing the configuration, K8s will automatically manage the layers below (creating new pods and destroying old ones, for example).

For viewing logs:

	kubectl logs POD_NAME

If it has not started yet, one can view the stage in the starting process by:

	kubectl describe POD_NAME

Enter a pod's terminal:

	kubectl exec -it POD_NAME -- /bin/bash

Delete a deployment (will also delete layers below):

	kubectl delete deployment NAME

In reality, we provide a configuration (yaml) to K8s instead of creating/editing the deployment (or other components) from the terminal. We can apply a configuration by using:

	kubectl apply -f CONF_FILE

We can also delete the component from the configuration file with:

	kubectl delete -f CONF_FILE 

K8s automatically creates or updates the deployments to match the configuration given.

### Configuration File

The configuration is devided in 3 parts:

- Metadata: Name, etc.
- Specification: Depending on the kind of component
- Status: **Generated and maintained by K8s**

K8s automatically updates the status based on the real values (from etcd). If it does not match the specification, it makes the necessary changes to make them match.

#### Deployment configuration

As said before, to manage pods we configure the deployment. A deployment's configuration looks like this:

```yaml
apiVersion: -
kind: Deployment
metadata:
	name: -
	labels: ...
# Until here it does not depend on the type
spec:
	replicas: 2
	selector: ...
	template: ...
```

The template defines the configuration of the pod, it has its own metadata and specification (like a configfile within a configfile).

#### Labels and selectors

It exists so the deployment knows which containers depend on it. The container defines a label and the deployment matches those pods with that label.

The deployment can also have a label, which it can be used by a Service (for example) to know which deployments it has to interact with.

#### Ports

Service has:
- port: The external one
- targetPot: The one that matches with the pod

Pod has:
- containerPort: Should match the targetPort

To get the ips assigned to pods:

	kubectl get pod -o wide
